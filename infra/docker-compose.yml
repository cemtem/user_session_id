services:
  postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - ../airflow_dir/pgdata:/var/lib/postgresql/data
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data:ro
    healthcheck:
      test: [ "CMD-SHELL","pg_isready -U airflow -d airflow" ]
      interval: 5s
      timeout: 5s
      retries: 10
    networks: [ sparknet ]

  init-perms:
    image: bitnami/minideb:bookworm
    user: "root"
    command: >
      bash -lc "
        apt-get update >/dev/null && apt-get install -y acl >/dev/null || true &&
        mkdir -p /opt/bitnami/spark/data_warehouse/bronze/events &&
        mkdir -p /opt/bitnami/spark/data_warehouse/silver/events &&
        mkdir -p /opt/bitnami/spark/data_warehouse/gold/events &&
        # Fix ownership for Spark user (1001:1001 in Bitnami Spark)
        chown -R 1001:1001 /opt/bitnami/spark/data_warehouse &&
        chmod -R 775 /opt/bitnami/spark/data_warehouse &&
        # Ensure ACLs allow default rwx for all
        setfacl -R -m u::rwx,g::rwx,o::rwx /opt/bitnami/spark/data_warehouse || true &&
        find /opt/bitnami/spark/data_warehouse -type d -exec setfacl -m d:u::rwx,d:g::rwx,d:o::rwx {} + || true
      "
    volumes:
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data
    networks: [ sparknet ]

  airflow-init:
    build:
      context: .
      dockerfile: airflow.Dockerfile
    image: airflow-with-spark:2.10.2-3.5.1
    user: "root"
    depends_on:
      postgres:
        condition: service_healthy
      init-perms:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
      _AIRFLOW_WWW_USER_FIRSTNAME: Admin
      _AIRFLOW_WWW_USER_LASTNAME: User
      _AIRFLOW_WWW_USER_EMAIL: admin@example.com
    command: version
    volumes:
      - ../airflow_dir/dags:/opt/airflow/dags
      - ../airflow_dir/logs:/opt/airflow/logs
      - ../airflow_dir/plugins:/opt/airflow/plugins
      - ../spark_app:/opt/spark/apps/spark_app
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data:ro
    networks: [ sparknet ]

  airflow-webserver:
    image: airflow-with-spark:2.10.2-3.5.1
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    ports: [ "8090:8080" ]
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: "8080"
      AIRFLOW__WEBSERVER__SECRET_KEY: replace_me

      PYTHONPATH: /opt/spark/apps
      PYSPARK_PYTHON: python3
      PYSPARK_DRIVER_PYTHON: python3
      WAREHOUSE_DIR: /opt/bitnami/spark/data_warehouse
      METADATA_DIR: /opt/spark/apps/spark_app/metadata
      SOURCE_DIR: /opt/source_data/events
    command: >
      bash -lc "
      airflow db migrate &&
      airflow users create --username airflow --password airflow --firstname Admin --lastname User --role Admin --email admin@example.com || true &&
      (airflow connections get spark_cluster >/dev/null 2>&1 ||
       airflow connections add spark_cluster --conn-type spark --conn-host spark://spark-master --conn-port 7077
      ) &&
      exec airflow webserver
      "
    volumes:
      - ../airflow_dir/dags:/opt/airflow/dags
      - ../airflow_dir/logs:/opt/airflow/logs
      - ../airflow_dir/plugins:/opt/airflow/plugins
      - ../spark_app:/opt/spark/apps/spark_app
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data:ro
    healthcheck:
      test: [ "CMD-SHELL", "curl -f http://localhost:8080/health || exit 1" ]
      interval: 10s
      timeout: 10s
      retries: 10
    networks: [ sparknet ]

  airflow-scheduler:
    image: airflow-with-spark:2.10.2-3.5.1
    ports:
      - "4040:4040"
      - "4041:4041"
    depends_on:
      airflow-webserver:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
      PYTHONPATH: /opt/spark/apps
      PYSPARK_PYTHON: python3
      PYSPARK_DRIVER_PYTHON: python3
      WAREHOUSE_DIR: /opt/bitnami/spark/data_warehouse
      METADATA_DIR: /opt/spark/apps/spark_app/metadata
      SOURCE_DIR: /opt/source_data/events

    command: >
      bash -lc "
      airflow db migrate &&
      (airflow connections get spark_cluster >/dev/null 2>&1 ||
       airflow connections add spark_cluster --conn-type spark --conn-host spark://spark-master --conn-port 7077
      ) &&
      exec airflow scheduler
      "
    volumes:
      - ../airflow_dir/dags:/opt/airflow/dags
      - ../airflow_dir/logs:/opt/airflow/logs
      - ../airflow_dir/plugins:/opt/airflow/plugins
      - ../spark_app:/opt/spark/apps/spark_app
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data:ro
    networks: [ sparknet ]

  spark-master:
    depends_on:
      init-perms:
        condition: service_completed_successfully
    image: bitnami/spark:3.5.1
    user: "root"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - WAREHOUSE_DIR=/opt/bitnami/spark/data_warehouse
      - METADATA_DIR=/opt/spark/apps/spark_app/metadata
      - SPARK_DRIVER_BIND_ADDRESS=0.0.0.0
    ports:
      - "7077:7077"
      - "9090:8080"
    volumes:
      - ../spark_app:/opt/spark/apps/spark_app
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data:ro
    networks: [ sparknet ]

  spark-worker-1:
    image: bitnami/spark:3.5.1
    depends_on: [ spark-master ]
    user: "root"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4G
      - METADATA_DIR=/opt/spark/apps/spark_app/metadata
    ports:
      - "9091:8081"
    volumes:
      - ../spark_app:/opt/spark/apps/spark_app
      - ../data_warehouse:/opt/bitnami/spark/data_warehouse
      - ../source_data:/opt/source_data:ro
    networks: [ sparknet ]

networks:
  sparknet:
    driver: bridge
